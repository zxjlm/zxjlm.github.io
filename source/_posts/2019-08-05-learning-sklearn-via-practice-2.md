---
layout: post
cid: 63
title: sklearn应用性学习(2)
slug: 63
date: 2019/08/05 18:58:00
updated: 2019/08/05 18:59:20
status: publish
author: harumonia
categories:
  - 源流清泉
  - Python
tags:
  - sklearn
thumb:
thumbStyle: default
hidden: false
---

应用性学习置于构造性学习之后，在理解 sklearn 的构造，各种参数的理论依据的基础上，进行实践性的学习。

本篇主要针对监督学习进行浅尝。

<!-- more -->

# 监督学习

- 分类
  与聚类算法被统一封装在 sklearn.cluster 模块不同，sklearn 库中的分类算法并未被统一 封装在一个子模块中，因此对分类算法的 import 方式各有不同。  
  **Sklearn 提供的分类函数包括** :  
  • k 近邻(knn)  
  • 朴素贝叶斯(naivebayes)，  
  • 支持向量机(svm)，  
  • 决策树 (decision tree)  
  • 神经网络模型(Neural networks)等  
  • 这其中有线性分类器，也有非线性分类器。

- 回归
  统计学分析数据的方法，目的在于了解两个或多个变数间是否相关、 研究其相关方向与强度，并建立数学模型以便观察特定变数来预测研究者感兴 趣的变数。回归分析可以帮助人们了解在自变量变化时因变量的变化量。一般 来说，通过回归分析我们可以由给出的自变量估计因变量的条件期望。  
  **Sklearn 提供的回归函数主要被封装在两个子模块中** ，分别是 sklearn.linear_model 和 sklearn.preprocessing。 sklearn.linear_modlel 封装的是一些线性函数，线性回归函数包括有:  
  • 普通线性回归函数( LinearRegression ) • 岭回归(Ridge)  
  • Lasso(Lasso)  
  非线性回归函数，如多项式回归(PolynomialFeatures)则通过 sklearn.preprocessing 子模块进行调用

## 分类

### KNN

在 sklearn 库中，可以使用 sklearn.neighbors.KNeighborsClassifier 创建一个 K 近邻分类器，主要参数有:
• n_neighbors:用于指定分类器中 K 的大小(默认值为 5，注意与 kmeans 的区别)  
• weights:设置选中的 K 个点对分类结果影响的权重(默认值为平均 权重“uniform”，可以选择“distance”代表越近的点权重越高， 或者传入自己编写的以距离为参数的权重计算函数)  
• algorithm:设置用于计算临近点的方法，因为当数据量很大的情况下计算当前点和所有点的距离再选出最近的 k 各点，这个计算量是很 费时的，所以(选项中有 ball_tree、kd_tree 和 brute，分别代表不 同的寻找邻居的优化算法，默认值为 auto，根据训练数据自动选择)

#### 使用经验

在实际使用时，我们可以使用所有训练数据构成特征 X 和标签 y，使用 fit() 函数进行训练。在正式分类时，通过一次性构造测试集或者一个一个输入 样本的方式，得到样本对应的分类结果。有关 K 的取值:  
• 如果较大，相当于使用较大邻域中的训练实例进行预测，可以减小估计误差，但是距离较远的样本也会对预测起作用，导致预测错误。  
• 相反地，如果 K 较小，相当于使用较小的邻域进行预测，如果邻居恰好是噪声点，会导致过拟合。  
• 一般情况下，K 会倾向选取较小的值，并使用交叉验证法选取最优 K 值。

### 决策树

在 sklearn 库中，可以使用 sklearn.tree.DecisionTreeClassifier 创 建一个决策树用于分类，其主要参数有:  
• criterion :用于选择属性的准则，可以传入“gini”代表基尼 系数，或者“entropy”代表信息增益。  
• max_features :表示在决策树结点进行分裂时，从多少个特征 中选择最优特征。可以设定固定数目、百分比或其他标准。它 的默认值是使用所有特征个数。

### 朴素贝叶斯

朴素贝叶斯分类器是一个以贝叶斯定理为基础  
的多分类的分类器。  
对于给定数据，首先基于特征的条件独立性假 设，学习输入输出的联合概率分布，然后基于此模 型，对给定的输入 x，利用贝叶斯定理求出后验概 率最大的输出 y。

在 sklearn 库中，实现了三个朴素贝叶斯分类器，如下表所示:  
分类器| 描述
--|---
naive_bayes.GussianNB |高斯朴素贝叶斯
naive_bayes.MultinomialNB |针对多项式模型的朴素贝叶斯分类器
naive_bayes.BernoulliNB | 针对多元伯努利模型的朴素贝叶斯分类器

区别在于假设某一特征的所有属于某个类别的观测值符合特定分布，如，分 类问题的特征包括人的身高，身高符合高斯分布，这类问题适合高斯朴素贝叶斯

在 sklearn 库中，可以使用 sklearn.naive_bayes.GaussianNB 创建一个高斯 朴素贝叶斯分类器，其参数有:

• priors :给定各个类别的先验概率。  
如果为空，则按训练数据的实际情况 进行统计;如果给定先验概率，则在训练过程中不能更改。

### 拓展

**在所有的特征数据中,可能存在缺失值或者冗余特征,如果将这些特征不加处理的送入后续的计算,可能会导致模型准确度下降并且增大计算量**

可以借助 weka 将数据进行可视化并统计

## 回归

### 线性回归

线性回归(Linear Regression)是利用数理统计中回归分析,来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。  
线性回归利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为简单 回归,大于一个自变量情况的叫做多元回归。

### 多项式回归

在一元回归分析中，如果依变量 y 与自变量 x 的关系为非线性的，但 是又找不到适当的函数曲线来拟合，则可以采用一元多项式回归。  
多项式回归的最大优点就是 **可以通过增加 x 的高次项对实测点进行逼 近，直至满意为止** 。  
事实上，多项式回归可以处理相当一类非线性问题，它在回归分析 中占有重要的地位，因为 _任一函数都可以分段用多项式来逼近_ 。

### 岭回归

**传统的基于最小二乘的线性回归法缺乏稳定性。**
岭回归(ridge regression)是一种专用于共线性数据分析的有偏估计回归方 法
是一种改良的最小二乘估计法，对某些数据的拟合要强于最小二乘法。

在 sklearn 库中，可以使用 sklearn.linear_model.Ridge 调用岭回归模型，其 主要参数有:  
• alpha:正则化因子，对应于损失函数中的
