<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.1.1"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"><link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/pace/1.0.2/themes/silver/pace-theme-center-atom.min.css"><script src="//cdn.bootcdn.net/ajax/libs/pace/1.0.2/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"blog.harumonia.moe",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!0,lazyload:!0,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="构造性学习中，以理论为主，并结合一定的例子 本篇是机器学习的入门篇，主体的脉络差不多近似于 《数据挖掘导论》 一书"><meta property="og:type" content="article"><meta property="og:title" content="sklearn构造性学习(1)"><meta property="og:url" content="https://blog.harumonia.moe/2019-08-02-learning-sklearn-via-theory-1/index.html"><meta property="og:site_name" content="Zaxon"><meta property="og:description" content="构造性学习中，以理论为主，并结合一定的例子 本篇是机器学习的入门篇，主体的脉络差不多近似于 《数据挖掘导论》 一书"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://harumona-blog.oss-cn-beijing.aliyuncs.com/old_articles/4253159243.png?Expires=1602314295"><meta property="og:image" content="https://harumona-blog.oss-cn-beijing.aliyuncs.com/old_articles/2299684497.png?Expires=1602314397"><meta property="article:published_time" content="2019-08-02T08:41:00.000Z"><meta property="article:modified_time" content="2019-08-05T18:59:29.000Z"><meta property="article:author" content="harumonia"><meta property="article:tag" content="sklearn"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://harumona-blog.oss-cn-beijing.aliyuncs.com/old_articles/4253159243.png?Expires=1602314295"><link rel="canonical" href="https://blog.harumonia.moe/2019-08-02-learning-sklearn-via-theory-1/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>sklearn构造性学习(1) | Zaxon</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><script>fetch("https://v1.hitokoto.cn/?c=i").then(function(t){return t.json()}).then(function(t){var o=document.getElementById("hitokoto"),n=document.getElementById("hitofrom");o.innerHTML=t.hitokoto,n.innerHTML="--- "+t.from}).catch(function(t){console.error(t)})</script></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Zaxon</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Find the key of soul</p><p class="site-hit" id="hitokoto"></p><p class="site-hit" id="hitofrom"></p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-fab fa-home"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-fab fa-user"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-fab fa-tags"></i>标签<span class="badge">57</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-fab fa-th"></i>分类<span class="badge">19</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-fab fa-archive"></i>归档<span class="badge">107</span></a></li><li class="menu-item menu-item-book"><a href="/books/" rel="section"><i class="fa fa-fw fa-fab fa-book"></i>图书</a></li><li class="menu-item menu-item-movie"><a href="/movies/" rel="section"><i class="fa fa-fw fa-fab fa-film"></i>电影</a></li><li class="menu-item menu-item-game"><a href="/games/" rel="section"><i class="fa fa-fw fa-fab fa-gamepad"></i>游戏</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/zxjlm" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://blog.harumonia.moe/2019-08-02-learning-sklearn-via-theory-1/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="https://harumona-blog.oss-cn-beijing.aliyuncs.com/blog/Ocabe.webp"><meta itemprop="name" content="harumonia"><meta itemprop="description" content="lazy"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Zaxon"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">sklearn构造性学习(1)</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-08-02 08:41:00" itemprop="dateCreated datePublished" datetime="2019-08-02T08:41:00+00:00">2019-08-02</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2019-08-05 18:59:29" itemprop="dateModified" datetime="2019-08-05T18:59:29+00:00">2019-08-05</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%BA%90%E6%B5%81%E6%B8%85%E6%B3%89/" itemprop="url" rel="index"><span itemprop="name">源流清泉</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%BA%90%E6%B5%81%E6%B8%85%E6%B3%89/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a> </span></span><span id="/2019-08-02-learning-sklearn-via-theory-1/" class="post-meta-item leancloud_visitors" data-flag-title="sklearn构造性学习(1)" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/2019-08-02-learning-sklearn-via-theory-1/#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2019-08-02-learning-sklearn-via-theory-1/" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>6.5k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>20 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>构造性学习中，以理论为主，并结合一定的例子</p><p>本篇是机器学习的入门篇，主体的脉络差不多近似于 《数据挖掘导论》 一书</p><a id="more"></a><h1 id="K-近邻算法"><a href="#K-近邻算法" class="headerlink" title="K 近邻算法"></a>K 近邻算法</h1><p><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier">主要函数说明</a><br>k 近邻算法是非常特殊的,可以认为是没有模型,或者说训练集本身就是模型<br>寻找 k 个近邻,来判断 x 点<br>主要解决分类问题</p><h2 id="计算距离"><a href="#计算距离" class="headerlink" title="计算距离"></a>计算距离</h2><h3 id="欧拉距离"><a href="#欧拉距离" class="headerlink" title="欧拉距离"></a>欧拉距离</h3><p>$$<br>\sqrt{\sum_{i=1}^{n}\left(X_{i}^{(a)}-X_{i}^{(b)}\right)^{2}}<br>$$</p><h3 id="曼哈顿距离"><a href="#曼哈顿距离" class="headerlink" title="曼哈顿距离"></a>曼哈顿距离</h3><p><img data-src="https://harumona-blog.oss-cn-beijing.aliyuncs.com/old_articles/4253159243.png?Expires=1602314295" alt="p1"><br>绿线:欧拉距离</p><h3 id="总结-闵可夫斯基距离"><a href="#总结-闵可夫斯基距离" class="headerlink" title="总结:闵可夫斯基距离"></a>总结:闵可夫斯基距离</h3><p>$$<br>\left(\sum_{i=1}^{n}\left|X_{i}^{(a)}-X_{i}^{(b)}\right|^{p}\right)^{\frac{1}{p}}<br>$$</p><h2 id="准确度"><a href="#准确度" class="headerlink" title="准确度"></a>准确度</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">accuracy_score(y_test,y_predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者直接从训练结果中导出准确度</span></span><br><span class="line">knn_clf.score(X_test)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h2><p>超参数:在算法运行前需要决定的参数<br>模型参数:算法过程中学习的参数</p><p>KNN 算法没有模型参数<br>kNN 算法中的 k 是典型的超参数</p><p>如何寻找好的超参数:</p><pre><code>领域知识
经验数值
实验搜索</code></pre><blockquote><p>除了 k 之外还有一个重要的超参数</p></blockquote><p><strong>k 与各个投票点的距离</strong><br>好处:<br>使得模型更加科学<br>解决了平票的情况</p><p>使用方法:加入 <strong>weights</strong> 参数</p><p>由上述的闵可夫斯基距离,得到又一个超参数 <strong>p</strong>,用来判断使用的距离公式</p><h2 id="技术实现"><a href="#技术实现" class="headerlink" title="技术实现"></a>技术实现</h2><p>使用 np.sum 对 np 进行求和<br>使用列表推导式进行列表的遍历<br>使用 collection 的 Counter 的 Counter 类来计算”投票”的结果</p><h2 id="数据归一化"><a href="#数据归一化" class="headerlink" title="数据归一化"></a>数据归一化</h2><p>作用:将所有的数据都映射到同一个尺度中<br><strong>最值归一化</strong>:将所有的数据都映射到 0-1 之间<br>适用于分布有明显边界的情况(如像素 0-255,学生的考试分数 0-100)</p><h3 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h3><p><strong>均值方差归一化 standardization</strong>:把所有的数据归一到均值为 0,方差为 1 的分布中<br>适用于数据分布没有明显的边界;有可能存在极端数据值<br><em>一般情况下使用</em></p><p>$$<br>x_{\text {scale}}=\frac{x-x_{\text {mean}}}{s}<br>$$</p><h3 id="如何对测试数据集进行归一化"><a href="#如何对测试数据集进行归一化" class="headerlink" title="如何对测试数据集进行归一化"></a>如何对测试数据集进行归一化</h3><p>与训练集的归一化方法不同</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(x_test - mean\_train ) &#x2F; std\_train</span><br></pre></td></tr></table></figure><p>原因:</p><ul><li>真实环境很可能无法得到测试数据的均值和方差(如只给你一个数据)</li><li>对数据的归一化也是算法的一部分</li></ul><p><img data-src="https://harumona-blog.oss-cn-beijing.aliyuncs.com/old_articles/2299684497.png?Expires=1602314397" alt="p2"></p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br></pre></td></tr></table></figure><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><ul><li>效率低下</li><li>高度数据相关(对 outlier 更加敏感)</li><li>预测的结果不具有可解释性</li></ul><h2 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h2><p>secore() 进行准确率对比</p><h1 id="线性回归算法"><a href="#线性回归算法" class="headerlink" title="线性回归算法"></a>线性回归算法</h1><p>目标:找到 a 和 b,使</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\displaystyle\sum_&#123;i&#x3D;1&#125;^ny\raisebox&#123;0.2em&#125;&#123;(i)&#125; -ax\raisebox&#123;0.2em&#125;&#123;(i)&#125; +b</span><br></pre></td></tr></table></figure><p>尽可能小<br>以上公式即 <strong>损失函数(lossfunction)</strong><br>部分函数中用上式计算拟合的程度,所以也称 <strong>效用函数(utility function)</strong><br>统称为 <strong>目标函数</strong></p><p>总结:</p><pre><code>通过分析问题,确定问题的损失函数或者效用函数
通过最优化损失函数或效用函数,获得机器学习的模型</code></pre><h2 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h2><p>一般情况下,向量化相较于普通法快很多</p><h2 id="评价-1"><a href="#评价-1" class="headerlink" title="评价"></a>评价</h2><p>均方误差 MSE<br>改进:<br>均方根误差 RMSE 使其对量纲更加敏感<br><em>上下的误差为均方根误差</em><br><strong>sklearn 中没有包装 RMSE</strong><br>平均绝对误差 MAE</p><p>$$<br>R M S E=\sqrt{\frac{1}{m} \sum_{i=1}^{m}\left(y_{t e s t}^{(i)}-\hat{y}_{t e s t}^{(i)}\right)^{2}}<br>$$</p><p>$$<br>M A E=\frac{1}{m} \sum_{i=1}^{m}\left|y_{t e s t}^{(i)}-\hat{y}_{t e s t}^{(i)}\right|<br>$$</p><h2 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h2><h3 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">lin_reg = LinearRegression()</span><br><span class="line">lin_reg.fit(X_train,y_train)</span><br><span class="line">lin_reg.coef_      <span class="comment">#theta</span></span><br><span class="line">lin_reg.intercept_    <span class="comment">#截距</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h1><p>基于搜索的最优化方法<br>作用: 最小化一个损失函数<br>梯度上升法 : 最大化一个效用函数</p><h2 id="深入"><a href="#深入" class="headerlink" title="深入"></a>深入</h2><ul><li>批量梯度下降法</li><li>随机梯度下降法</li><li>小批量梯度下降法</li></ul><h2 id="线性回归中使用梯度下降法"><a href="#线性回归中使用梯度下降法" class="headerlink" title="线性回归中使用梯度下降法"></a>线性回归中使用梯度下降法</h2><p>在使用梯度下降法时,对目标函数要进行特殊的设计<br>虽然理论上挡土度非常大的时候,可以通过调节 eta 来得到想要的结果,但是会影响效率</p><h2 id="梯度下降的向量化和标准化"><a href="#梯度下降的向量化和标准化" class="headerlink" title="梯度下降的向量化和标准化"></a>梯度下降的向量化和标准化</h2><p>向量化:</p><pre><code>简化了公式</code></pre><p>标准化:</p><pre><code>在使用梯度下降法之前,将数据归一化</code></pre><p>梯度下降法相比标准方程在进行大数据处理时具有明显优势</p><h2 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h2><p>用精度换时间<br>学习率逐渐递减</p><h2 id="scikit-实现"><a href="#scikit-实现" class="headerlink" title="scikit 实现"></a>scikit 实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 使用波士顿房价数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">boston = datasets.load_boston()</span><br><span class="line">X = boston.data</span><br><span class="line">y = boston.target</span><br><span class="line"></span><br><span class="line">X = X[y &lt; <span class="number">50.0</span>]</span><br><span class="line">y = y[y &lt; <span class="number">50.0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型分类</span></span><br><span class="line"><span class="keyword">from</span> playML.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, seed=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 归一化</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">standardScaler = StandardScaler()</span><br><span class="line">standardScaler.fit(X_train)</span><br><span class="line">X_train_standard = standardScaler.transform(X_train)</span><br><span class="line">X_test_standard = standardScaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 正式使用scikit中的SDG</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line"></span><br><span class="line">sgd_reg = SGDRegressor()</span><br><span class="line">%time sgd_reg.fit(X_train_standard, y_train)</span><br><span class="line">sgd_reg.score(X_test_standard, y_test)</span><br><span class="line"></span><br><span class="line">sgd_reg = SGDRegressor(n_iter=<span class="number">50</span>)</span><br><span class="line">%time sgd_reg.fit(X_train_standard, y_train)</span><br><span class="line">sgd_reg.score(X_test_standard, y_test)</span><br></pre></td></tr></table></figure><h1 id="主成分分析-PCA"><a href="#主成分分析-PCA" class="headerlink" title="主成分分析 PCA"></a>主成分分析 PCA</h1><ul><li>一个非监督的机器学习算法</li><li>主要用于数据降维</li><li>通过降维,可以发现更便于人类理解的特征</li><li>其他应用:可视化;去噪</li></ul><blockquote><p>找到一个轴,所有的点映射到这个轴之后方差最大</p></blockquote><p>S1:将样例的均值归零(作用是化简公式)(<strong>demean</strong>)<br>S2:求轴的方向 w=(w1,w2),使得所有的样本,映射到 w 以后,!</p><p><strong>区分 PAC 和线性回归</strong></p><h2 id="梯度上升法解决主成分分析问题"><a href="#梯度上升法解决主成分分析问题" class="headerlink" title="梯度上升法解决主成分分析问题"></a>梯度上升法解决主成分分析问题</h2><p>注意:</p><ul><li>每次求一个单位方向</li><li>不能用 0 向量开始</li><li>不能使用 StandardScaler 标准化数据</li></ul><h2 id="实现-2"><a href="#实现-2" class="headerlink" title="实现"></a>实现</h2><p>S1:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">demean</span>(<span class="params">X</span>):</span></span><br><span class="line">    <span class="keyword">return</span> X - np.mean(X, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h2 id="高维数据映射为低维"><a href="#高维数据映射为低维" class="headerlink" title="高维数据映射为低维"></a>高维数据映射为低维</h2><p>低维的数据是可以返回到高维的, <strong>但是存在缺损</strong></p><h2 id="scikit-实现-1"><a href="#scikit-实现-1" class="headerlink" title="scikit 实现"></a>scikit 实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">pca = PCA(<span class="number">0.95</span>) <span class="comment"># 锁定精度</span></span><br><span class="line">pca.fit(X_train)</span><br><span class="line"></span><br><span class="line">pca.n_components_ <span class="comment">#依据X_train确定保留的维度</span></span><br><span class="line"></span><br><span class="line">knn_clf = KNeighborsClassifier() <span class="comment"># 降维后进行分类</span></span><br><span class="line">knn_clf.fit(X_train_reduction, y_train)</span><br><span class="line"></span><br><span class="line">knn_clf.score(X_test_reduction, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">pca.fit(X)</span><br><span class="line">X_reduction = pca.transform(X)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    plt.scatter(X_reduction[y==i,<span class="number">0</span>], X_reduction[y==i,<span class="number">1</span>], alpha=<span class="number">0.8</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><strong>PCA 在降维的过程中还有降噪的作用,降噪的结果就是,维度降低,但是准确率反而提升了</strong></p><h1 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h1><p>实际中,大多数的数据都是非线性的关系</p><ul><li><p>PolynomialFeatures(degeree=3)<br>生成三次多项式<br>个数为 10<br>一次 3 1,x1,x2<br>二次 3 x1^2,x2^2,x1*x2<br>三次 4 x1^3,x2^3,x1^2*x2,x1*x2^2</p></li><li><p>pipeline</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">x = np.random.uniform(<span class="number">-3</span>, <span class="number">3</span>, size=<span class="number">100</span>)</span><br><span class="line">X = x.reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">0.5</span> * x**<span class="number">2</span> + x + <span class="number">2</span> + np.random.normal(<span class="number">0</span>, <span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">poly_reg = Pipeline([</span><br><span class="line">    (<span class="string">&quot;poly&quot;</span>, PolynomialFeatures(degree=<span class="number">2</span>)),</span><br><span class="line">    (<span class="string">&quot;std_scaler&quot;</span>, StandardScaler()),</span><br><span class="line">    (<span class="string">&quot;lin_reg&quot;</span>, LinearRegression())</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">poly_reg.fit(X, y)</span><br><span class="line">y_predict = poly_reg.predict(X)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.plot(np.sort(x), y_predict[np.argsort(x)], color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h2><p>过拟合:训练测试集上表现很好,但在测试数据集上表现不好</p><h3 id="模型的泛化能力"><a href="#模型的泛化能力" class="headerlink" title="模型的泛化能力"></a>模型的泛化能力</h3><p>使用训练数据集和测试数据集</p><blockquote><p>测试数据集的意义就是评估模型的泛化能力</p></blockquote><h2 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learning_curve</span>(<span class="params">algo, X_train, X_test, y_train, y_test</span>):</span></span><br><span class="line">    train_score = []</span><br><span class="line">    test_score = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(X_train)+<span class="number">1</span>):</span><br><span class="line">        algo.fit(X_train[:i], y_train[:i])</span><br><span class="line"></span><br><span class="line">        y_train_predict = algo.predict(X_train[:i])</span><br><span class="line">        train_score.append(mean_squared_error(y_train[:i], y_train_predict))</span><br><span class="line"></span><br><span class="line">        y_test_predict = algo.predict(X_test)</span><br><span class="line">        test_score.append(mean_squared_error(y_test, y_test_predict))</span><br><span class="line"></span><br><span class="line">    plt.plot([i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(X_train)+<span class="number">1</span>)],</span><br><span class="line">                               np.sqrt(train_score), label=<span class="string">&quot;train&quot;</span>)</span><br><span class="line">    plt.plot([i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(X_train)+<span class="number">1</span>)],</span><br><span class="line">                               np.sqrt(test_score), label=<span class="string">&quot;test&quot;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.axis([<span class="number">0</span>, len(X_train)+<span class="number">1</span>, <span class="number">0</span>, <span class="number">4</span>])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plot_learning_curve(LinearRegression(), X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure><blockquote><p>注意,针对特定测试数据集的过拟合<br>增加 验证数据集(调整超参数使用的数据集)</p></blockquote><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line">knn_clf = KNeighborsClassifier()</span><br><span class="line">cross_val_score(knn_clf, X_train, y_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">best_k, best_p, best_score = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">11</span>):</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">        knn_clf = KNeighborsClassifier(weights=<span class="string">&quot;distance&quot;</span>, n_neighbors=k, p=p)</span><br><span class="line">        scores = cross_val_score(knn_clf, X_train, y_train)</span><br><span class="line">        score = np.mean(scores)</span><br><span class="line">        <span class="keyword">if</span> score &gt; best_score:</span><br><span class="line">            best_k, best_p, best_score = k, p, score</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;Best K =&quot;</span>, best_k)</span><br><span class="line">print(<span class="string">&quot;Best P =&quot;</span>, best_p)</span><br><span class="line">print(<span class="string">&quot;Best Score =&quot;</span>, best_score)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">best_knn_clf = KNeighborsClassifier(weights=<span class="string">&quot;distance&quot;</span>, n_neighbors=<span class="number">2</span>, p=<span class="number">2</span>)</span><br><span class="line">best_knn_clf.fit(X_train, y_train)</span><br><span class="line">best_knn_clf.score(X_test, y_test)</span><br></pre></td></tr></table></figure><p>将训练数据分为 k 份,训练 k 个模型,并进行交叉验证<br>最后求均值</p><h3 id="网格搜索"><a href="#网格搜索" class="headerlink" title="网格搜索"></a>网格搜索</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">param_grid = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&#x27;weights&#x27;</span>: [<span class="string">&#x27;distance&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;n_neighbors&#x27;</span>: [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">11</span>)],</span><br><span class="line">        <span class="string">&#x27;p&#x27;</span>: [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>)]</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">grid_search = GridSearchCV(knn_clf, param_grid, verbose=<span class="number">1</span>)</span><br><span class="line">grid_search.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><h3 id="留一法-LOO-CV"><a href="#留一法-LOO-CV" class="headerlink" title="留一法 LOO-CV"></a>留一法 LOO-CV</h3><p>把训练数据集分成 m 份,成为留一法</p><blockquote><p>Leabe-One_out Cross Validation</p></blockquote><p>优点:完全不受随机的印象,最接近模型真正的性能指标<br>缺点:计算量巨大</p><blockquote><p>论文中验证严谨性</p></blockquote><h3 id="偏差方差权衡-bias-variance"><a href="#偏差方差权衡-bias-variance" class="headerlink" title="偏差方差权衡 bias variance"></a>偏差方差权衡 bias variance</h3><p>模型误差 = 偏差+方差+不可避免的误差</p><h4 id="偏差-bias"><a href="#偏差-bias" class="headerlink" title="偏差(bias)"></a>偏差(bias)</h4><p>导致偏差的主要原因:<br>对问题本身的假设不正确<br>如:非线性数据使用线性回归</p><h4 id="方差-variance"><a href="#方差-variance" class="headerlink" title="方差(variance)"></a>方差(variance)</h4><p>数据的一点点扰动都会较大的影响模型<br>通常原因,使用的模型太复杂<br>如,高阶多项式回归</p><p><strong>机器学习的主要挑战来自于方差</strong></p><p>1.降低模型复杂度 2.减少数据维度;降噪 3.增加样本书 4.使用验证集</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>有一些算法天生是高方差算法. 如 KNN<br>非参数学习的通常都是高方差算法,因为不对数据进行人和假设</p><p>you 一些算法天生就是高偏差算法. 如线性回归<br>参数学习通常都是高偏差算法,因为对数据具有极强的假设</p><p>大多数算法具有相应的参数,可以调整偏差和方差<br>如 kNN 中的 k\线性回归中使用多项式回归</p><p>偏差和方差是矛盾的<br>降低偏差会提高方差,反之亦然.</p><h3 id="模型正则化"><a href="#模型正则化" class="headerlink" title="模型正则化"></a>模型正则化</h3><p>$$<br>J(\theta)=M S E(y, \hat{y} ; \theta)+\alpha \frac{1}{2} \sum_{i=1}^{n} \theta_{i}^{2}<br>$$</p><h1 id="logistic-regression"><a href="#logistic-regression" class="headerlink" title="logistic regression"></a>logistic regression</h1><h2 id="解决多分类问题"><a href="#解决多分类问题" class="headerlink" title="解决多分类问题"></a>解决多分类问题</h2><p>将多分类问题简化为二分类问题</p><h3 id="OvR-One-vs-Rest"><a href="#OvR-One-vs-Rest" class="headerlink" title="OvR(One vs Rest)"></a>OvR(One vs Rest)</h3><p>n 个类别进行 n 次分类,选择分类得分最高的</p><h3 id="OvO-one-vs-one"><a href="#OvO-one-vs-one" class="headerlink" title="OvO(one vs one)"></a>OvO(one vs one)</h3><p>n 个类别进行 C(n,2)次分类,选择赢数最高的分类</p><p><strong>ovo 耗时更长,准确率更高</strong></p></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>harumonia</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://blog.harumonia.moe/2019-08-02-learning-sklearn-via-theory-1/" title="sklearn构造性学习(1)">https://blog.harumonia.moe/2019-08-02-learning-sklearn-via-theory-1/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/CC%20BY-NC-SA%204.0/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/sklearn/" rel="tag"><i class="fa fa-tag"></i> sklearn</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2019-07-30-questions-and-solutions-of-flask-deploy-2/" rel="prev" title="flask部署(2)"><i class="fa fa-chevron-left"></i> flask部署(2)</a></div><div class="post-nav-item"><a href="/2019-08-04-learning-sklearn-via-practice-1/" rel="next" title="sklearn应用性学习(1)">sklearn应用性学习(1) <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95"><span class="nav-number">1.</span> <span class="nav-text">K 近邻算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E8%B7%9D%E7%A6%BB"><span class="nav-number">1.1.</span> <span class="nav-text">计算距离</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AC%A7%E6%8B%89%E8%B7%9D%E7%A6%BB"><span class="nav-number">1.1.1.</span> <span class="nav-text">欧拉距离</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9B%BC%E5%93%88%E9%A1%BF%E8%B7%9D%E7%A6%BB"><span class="nav-number">1.1.2.</span> <span class="nav-text">曼哈顿距离</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-%E9%97%B5%E5%8F%AF%E5%A4%AB%E6%96%AF%E5%9F%BA%E8%B7%9D%E7%A6%BB"><span class="nav-number">1.1.3.</span> <span class="nav-text">总结:闵可夫斯基距离</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%87%86%E7%A1%AE%E5%BA%A6"><span class="nav-number">1.2.</span> <span class="nav-text">准确度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0"><span class="nav-number">1.3.</span> <span class="nav-text">超参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.4.</span> <span class="nav-text">技术实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">1.5.</span> <span class="nav-text">数据归一化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%94%B9%E8%BF%9B"><span class="nav-number">1.5.1.</span> <span class="nav-text">改进</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%AF%B9%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BF%9B%E8%A1%8C%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">1.5.2.</span> <span class="nav-text">如何对测试数据集进行归一化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.5.3.</span> <span class="nav-text">实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%BA%E7%82%B9"><span class="nav-number">1.6.</span> <span class="nav-text">缺点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%84%E4%BB%B7"><span class="nav-number">1.7.</span> <span class="nav-text">评价</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95"><span class="nav-number">2.</span> <span class="nav-text">线性回归算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E5%8C%96"><span class="nav-number">2.1.</span> <span class="nav-text">向量化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%84%E4%BB%B7-1"><span class="nav-number">2.2.</span> <span class="nav-text">评价</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">2.3.</span> <span class="nav-text">多元线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0-1"><span class="nav-number">2.3.1.</span> <span class="nav-text">实现</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-number">3.</span> <span class="nav-text">梯度下降法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%85%A5"><span class="nav-number">3.1.</span> <span class="nav-text">深入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%AD%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-number">3.2.</span> <span class="nav-text">线性回归中使用梯度下降法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%E5%92%8C%E6%A0%87%E5%87%86%E5%8C%96"><span class="nav-number">3.3.</span> <span class="nav-text">梯度下降的向量化和标准化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-number">3.4.</span> <span class="nav-text">随机梯度下降法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#scikit-%E5%AE%9E%E7%8E%B0"><span class="nav-number">3.5.</span> <span class="nav-text">scikit 实现</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-PCA"><span class="nav-number">4.</span> <span class="nav-text">主成分分析 PCA</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8A%E5%8D%87%E6%B3%95%E8%A7%A3%E5%86%B3%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98"><span class="nav-number">4.1.</span> <span class="nav-text">梯度上升法解决主成分分析问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0-2"><span class="nav-number">4.2.</span> <span class="nav-text">实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%AB%98%E7%BB%B4%E6%95%B0%E6%8D%AE%E6%98%A0%E5%B0%84%E4%B8%BA%E4%BD%8E%E7%BB%B4"><span class="nav-number">4.3.</span> <span class="nav-text">高维数据映射为低维</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#scikit-%E5%AE%9E%E7%8E%B0-1"><span class="nav-number">4.4.</span> <span class="nav-text">scikit 实现</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92"><span class="nav-number">5.</span> <span class="nav-text">多项式回归</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="nav-number">5.1.</span> <span class="nav-text">过拟合和欠拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B"><span class="nav-number">5.1.1.</span> <span class="nav-text">模型的泛化能力</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF"><span class="nav-number">5.2.</span> <span class="nav-text">学习曲线</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="nav-number">5.2.1.</span> <span class="nav-text">交叉验证</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2"><span class="nav-number">5.2.2.</span> <span class="nav-text">网格搜索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%95%99%E4%B8%80%E6%B3%95-LOO-CV"><span class="nav-number">5.2.3.</span> <span class="nav-text">留一法 LOO-CV</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%81%8F%E5%B7%AE%E6%96%B9%E5%B7%AE%E6%9D%83%E8%A1%A1-bias-variance"><span class="nav-number">5.2.4.</span> <span class="nav-text">偏差方差权衡 bias variance</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%81%8F%E5%B7%AE-bias"><span class="nav-number">5.2.4.1.</span> <span class="nav-text">偏差(bias)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%96%B9%E5%B7%AE-variance"><span class="nav-number">5.2.4.2.</span> <span class="nav-text">方差(variance)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">5.2.4.3.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">5.2.5.</span> <span class="nav-text">模型正则化</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#logistic-regression"><span class="nav-number">6.</span> <span class="nav-text">logistic regression</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="nav-number">6.1.</span> <span class="nav-text">解决多分类问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#OvR-One-vs-Rest"><span class="nav-number">6.1.1.</span> <span class="nav-text">OvR(One vs Rest)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#OvO-one-vs-one"><span class="nav-number">6.1.2.</span> <span class="nav-text">OvO(one vs one)</span></a></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="harumonia" src="https://harumona-blog.oss-cn-beijing.aliyuncs.com/blog/Ocabe.webp"><p class="site-author-name" itemprop="name">harumonia</p><div class="site-description" itemprop="description">lazy</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">107</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">19</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">57</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zxjlm" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zxjlm" rel="noopener" target="_blank"><i class="fa fa-fw fa-fab fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:zxjlm233@gmail.com" title="E-Mail → mailto:zxjlm233@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-fab fa-envelope"></i>E-Mail</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2017 – <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-fab fa-asterisk"></i> </span><span class="author" itemprop="copyrightHolder">harumonia</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">369k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">19:12</span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script><script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdnjs.cloudflare.com/ajax/libs/valine/1.3.10/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : true,
      appId      : 'MRbMQ94D5q51YRYIbsRF0kcH-gzGzoHsz',
      appKey     : 'A9JJBbjhfc9liCzCJgWIxpDa',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});</script><script async src="/js/cursor/fireworks.js"></script></body></html>