<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.1.1"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"><link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/pace/1.0.2/themes/silver/pace-theme-center-atom.min.css"><script src="//cdn.bootcdn.net/ajax/libs/pace/1.0.2/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"blog.harumonia.moe",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!0,lazyload:!0,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="本篇是笔者在学习 “Introduction to data mining”(《数据挖掘导论》)的英文最新版所记录的一些学习笔记。 2020.10.10 更新 很可惜的是，在多次迁移的过程中，本章的图片都丢失了。 食之无味，弃之可惜。所以暂且先保留在这里，以后再补全吧。"><meta property="og:type" content="article"><meta property="og:title" content="Introduction to data mining(1)"><meta property="og:url" content="https://blog.harumonia.moe/2019-08-17-Introduction_to_data_mining-1.html"><meta property="og:site_name" content="Zaxon"><meta property="og:description" content="本篇是笔者在学习 “Introduction to data mining”(《数据挖掘导论》)的英文最新版所记录的一些学习笔记。 2020.10.10 更新 很可惜的是，在多次迁移的过程中，本章的图片都丢失了。 食之无味，弃之可惜。所以暂且先保留在这里，以后再补全吧。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://www.harumonia.top/images/2019/08/17/image.md.png"><meta property="og:image" content="http://www.harumonia.top/images/2019/08/17/image04e5c16df6c5ea4b.md.png"><meta property="og:image" content="http://www.harumonia.top/images/2019/08/17/image1f4bfe8f69e2395b.md.png"><meta property="og:image" content="http://www.harumonia.top/images/2019/08/17/image59be1d41fb6ceff6.md.png"><meta property="og:image" content="http://www.harumonia.top/images/2019/08/17/imagea43b43449d9493db.md.png"><meta property="og:image" content="http://www.harumonia.top/images/2019/08/17/imageb9a8cc3bce2b2a12.md.png"><meta property="og:image" content="http://www.harumonia.top/images/2019/08/17/image418cf8e9aaebd1f7.md.png"><meta property="og:image" content="http://www.harumonia.top/images/2019/08/17/imagee59e593bbcfd3120.md.png"><meta property="og:image" content="http://www.harumonia.top/images/2019/08/17/image09c78ec0185a1ef1.png"><meta property="og:image" content="http://www.harumonia.top/images/2019/08/17/image4bfd8ac1d8b3156f.png"><meta property="og:image" content="http://www.harumonia.top/images/2019/08/17/imagec78fb1c028be231c.png"><meta property="og:image" content="http://www.harumonia.top/images/2019/08/17/image709b0bba0e810a01.png"><meta property="og:image" content="http://www.harumonia.top/images/2019/08/17/imagee4c8afe3e4e60ecf.png"><meta property="og:image" content="http://www.harumonia.top/images/2019/08/17/image950d41dec2581905.png"><meta property="og:image" content="http://www.harumonia.top/images/2019/08/17/image8002f990119f2a64.png"><meta property="og:image" content="http://www.harumonia.top/images/2019/08/17/imagefa2b122d3c3982db.png"><meta property="article:published_time" content="2019-08-17T09:14:20.000Z"><meta property="article:modified_time" content="2019-08-17T09:14:20.000Z"><meta property="article:author" content="harumonia"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://www.harumonia.top/images/2019/08/17/image.md.png"><link rel="canonical" href="https://blog.harumonia.moe/2019-08-17-Introduction_to_data_mining-1.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>Introduction to data mining(1) | Zaxon</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Zaxon</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">find the key of soul</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-fab fa-home"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-fab fa-user"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-fab fa-tags"></i>标签<span class="badge">53</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-fab fa-th"></i>分类<span class="badge">16</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-fab fa-archive"></i>归档<span class="badge">101</span></a></li><li class="menu-item menu-item-book"><a href="/books/" rel="section"><i class="fa fa-fw fa-fab fa-book"></i>图书</a></li><li class="menu-item menu-item-movie"><a href="/movies/" rel="section"><i class="fa fa-fw fa-fab fa-film"></i>电影</a></li><li class="menu-item menu-item-game"><a href="/games/" rel="section"><i class="fa fa-fw fa-fab fa-gamepad"></i>游戏</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/zxjlm" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://blog.harumonia.moe/2019-08-17-Introduction_to_data_mining-1.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="https://harumona-blog.oss-cn-beijing.aliyuncs.com/blog/Ocabe.webp"><meta itemprop="name" content="harumonia"><meta itemprop="description" content="find the key of soul"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Zaxon"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Introduction to data mining(1)</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-08-17 09:14:20" itemprop="dateCreated datePublished" datetime="2019-08-17T09:14:20+00:00">2019-08-17</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%BA%90%E6%B5%81%E6%B8%85%E6%B3%89/" itemprop="url" rel="index"><span itemprop="name">源流清泉</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%BA%90%E6%B5%81%E6%B8%85%E6%B3%89/dataMining/" itemprop="url" rel="index"><span itemprop="name">dataMining</span></a> </span></span><span id="/2019-08-17-Introduction_to_data_mining-1.html" class="post-meta-item leancloud_visitors" data-flag-title="Introduction to data mining(1)" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/2019-08-17-Introduction_to_data_mining-1.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2019-08-17-Introduction_to_data_mining-1.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>5.9k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>18 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>本篇是笔者在学习 “Introduction to data mining”(《数据挖掘导论》)的英文最新版所记录的一些学习笔记。</p><p>2020.10.10 更新</p><p>很可惜的是，在多次迁移的过程中，本章的图片都丢失了。</p><p>食之无味，弃之可惜。所以暂且先保留在这里，以后再补全吧。</p><a id="more"></a><h1 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h1><h2 id="什么是数据挖掘"><a href="#什么是数据挖掘" class="headerlink" title="什么是数据挖掘"></a>什么是数据挖掘</h2><p>数据挖掘是在大型数据库中，自动发现有用信息的过程。</p><blockquote><p>Data mining is the processof automatically discoveringuseful information in large data repositories.</p></blockquote><h3 id="数据挖掘与知识发现"><a href="#数据挖掘与知识发现" class="headerlink" title="数据挖掘与知识发现"></a>数据挖掘与知识发现</h3><p><a target="_blank" rel="noopener" href="http://www.harumonia.top/image/JVRe"><img data-src="http://www.harumonia.top/images/2019/08/17/image.md.png" alt="image.md.png"></a></p><h2 id="数据挖掘要解决的问题"><a href="#数据挖掘要解决的问题" class="headerlink" title="数据挖掘要解决的问题"></a>数据挖掘要解决的问题</h2><ul><li>可伸缩</li><li>高维性</li><li>异种数据和复杂数据</li><li>数据的所有权与分布</li><li>非传统的分析</li></ul><h2 id="数据挖掘的任务"><a href="#数据挖掘的任务" class="headerlink" title="数据挖掘的任务"></a>数据挖掘的任务</h2><ul><li>预测变量<blockquote><p>The objective of thesetasks is to predict the value of a par- ticular attribute basedon the valuesof other attributes. The attribute to be predicted is commonly known as the target or dependent vari- able, while the attributes used for making the prediction are known as the explanatory or independent variables.</p></blockquote></li><li>描述任务<blockquote><p>Here, the objective is to derive patterns (correlations, trends, clusters, trajectories, and anomalies) that summarize the un- derlying relationships in data. Descriptive data mining tasks are often exploratory in nature and frequently require postprocessingtechniques to validate and explain the results</p></blockquote></li></ul><h3 id="四大任务"><a href="#四大任务" class="headerlink" title="四大任务"></a>四大任务</h3><h4 id="预测建模"><a href="#预测建模" class="headerlink" title="预测建模"></a>预测建模</h4><ul><li>分类：用于预测 <strong>离散</strong> 的目标变量 (二值型)</li><li>回归：…… <strong>连续</strong> ……</li></ul><p><em>应用</em> ：可以用来确定顾客对产品促销的反应、预测地球生态系统的扰动、根据检查结果判断病人是否具有某种疾病<br><em>典型案例</em> ：预测花的类型</p><h4 id="关系分析"><a href="#关系分析" class="headerlink" title="关系分析"></a>关系分析</h4><p>用来发现描述数据中强关联特征的模式<br><em>应用</em> : 找出具有相关功能的基因组、识别用户一起访问的 Web 页面、理解地球气候系统不同元素之间的联系<br><em>典型案例</em> : 购物篮分析(牛奶——尿布)</p><h4 id="聚类分析"><a href="#聚类分析" class="headerlink" title="聚类分析"></a>聚类分析</h4><p>发现紧密相关的 <strong>观测值组群</strong> ，使得与属于不同簇的观测值相比,属于同一簇的观测值尽可能地相似.</p><p><em>应用</em> : 对相关的客户分组 , 找出显著影响地球气候的海洋区域以及压缩数据等<br><em>典型案例</em> : 文档聚类(词之间的相似性)</p><h4 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a>异常检测</h4><p>识别特征显著不同于其他数据的观测值,这样的值称为 <strong>异常点</strong> 或 <strong>离群点</strong> .<br>好的异常检测器必然具有高检测率和低误报率.</p><p><em>应用</em> : 检测欺诈\网络攻击\疾病的不寻常模式\生态系统扰动<br><em>典型案例</em> :信用卡欺诈检测</p><h1 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h1><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><p>数据对象 : 记录\点\向量\模式\事件\案例\样本\观测或实体</p><h4 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h4><p>四种属性类型:标称 序数 区间 比率<br><a target="_blank" rel="noopener" href="http://www.harumonia.top/image/N7Zp"><img data-src="http://www.harumonia.top/images/2019/08/17/image04e5c16df6c5ea4b.md.png" alt="image04e5c16df6c5ea4b.md.png"></a><br>标称和序数统称 <strong>分类的</strong> (categorical) 或 <strong>定性的</strong> (qualitative) 属性.顾名思义,定性属性(如雇员 ID)不具有数的大部分性质,应当像对待符号一样对待他们.其余两种属性,即区间和比率,统称 <strong>定量的</strong> (quantitative) 或 <strong>数值的</strong> (numeric) 属性,用数表示,并且具有数的大部分性质.</p><pre><code>                        定义属性层次的变换</code></pre><p><a target="_blank" rel="noopener" href="http://www.harumonia.top/image/NNYg"><img data-src="http://www.harumonia.top/images/2019/08/17/image1f4bfe8f69e2395b.md.png" alt="image1f4bfe8f69e2395b.md.png"></a></p><h3 id="数据集的类型"><a href="#数据集的类型" class="headerlink" title="数据集的类型"></a>数据集的类型</h3><ul><li>记录数据</li><li>基于图形的数据</li><li>有序的数据</li></ul><h4 id="数据集的一般特性"><a href="#数据集的一般特性" class="headerlink" title="数据集的一般特性"></a>数据集的一般特性</h4><ul><li>维度<br>对象具有的属性的数目.分析高维数据有时会陷入 <strong>维灾难(curse of dimensionality)</strong> 所以要先预处理.</li><li>稀疏性</li><li>分辨率</li></ul><h4 id="记录数据"><a href="#记录数据" class="headerlink" title="记录数据"></a>记录数据</h4><ul><li>事物数据或购物篮数据</li><li>数据矩阵</li><li>稀疏数据矩阵</li></ul><p><a target="_blank" rel="noopener" href="http://www.harumonia.top/image/Nqn6"><img data-src="http://www.harumonia.top/images/2019/08/17/image59be1d41fb6ceff6.md.png" alt="image59be1d41fb6ceff6.md.png"></a></p><h4 id="基于图形的数据"><a href="#基于图形的数据" class="headerlink" title="基于图形的数据"></a>基于图形的数据</h4><p>这里考虑两种特殊情况:图形捕获数据对象之间的联系\数据对象本身用图形表示</p><h4 id="有序数据"><a href="#有序数据" class="headerlink" title="有序数据"></a>有序数据</h4><p>属性具有涉及时间或空间序的联系</p><ul><li>时序数据</li><li>序列数据</li><li>时间序列数据</li><li>空间数据</li></ul><h4 id="处理非记录数据"><a href="#处理非记录数据" class="headerlink" title="处理非记录数据"></a>处理非记录数据</h4><h2 id="数据质量"><a href="#数据质量" class="headerlink" title="数据质量"></a>数据质量</h2><p>(1)数据质量问题的检测和纠正<br>(2)使用可以容忍低质量数据的算法</p><h3 id="测量和数据收集问题"><a href="#测量和数据收集问题" class="headerlink" title="测量和数据收集问题"></a>测量和数据收集问题</h3><ul><li>噪声</li><li>伪象</li><li>偏倚</li><li>精度</li><li>准确率</li></ul><h4 id="测量误差和数据收集错误-Measurement-and-Data-Collection-Errors"><a href="#测量误差和数据收集错误-Measurement-and-Data-Collection-Errors" class="headerlink" title="测量误差和数据收集错误 Measurement and Data Collection Errors"></a>测量误差和数据收集错误 Measurement and Data Collection Errors</h4><p>通过人工干预来纠正(比如在数据录入的时候)</p><h4 id="噪声和伪象-Noise-and-Artifacts"><a href="#噪声和伪象-Noise-and-Artifacts" class="headerlink" title="噪声和伪象 Noise and Artifacts"></a>噪声和伪象 Noise and Artifacts</h4><p>噪声 Noise : 通常用于包含时间或空间分量的数据，这种情况下，通常可以使用信号或图像处理技术来降低噪声<br>伪象 Artifacts : 确定性失真</p><pre><code>鲁棒算法：
    在噪声干扰下也能产生可以接受的结果。</code></pre><h4 id="精度-偏倚和准确率-Precision-Bias-and-Accuracy"><a href="#精度-偏倚和准确率-Precision-Bias-and-Accuracy" class="headerlink" title="精度\偏倚和准确率 Precision, Bias, and Accuracy"></a>精度\偏倚和准确率 Precision, Bias, and Accuracy</h4><p>精度 Precision : (同一个量的)重复测量值之间的接近程度<br>偏倚 Bias : 测量值与被测量值之间的系统的变差<br>准确率 Accuracy :被测量的测量值与实际值之间的接近度</p><pre><code>准确度依赖于精度和偏倚</code></pre><p>异常值 Outliers</p><h4 id="离群点-Outliers"><a href="#离群点-Outliers" class="headerlink" title="离群点 Outliers"></a>离群点 Outliers</h4><p>与典型值不同的值，即“异常”<br>区别噪声和离群点</p><h4 id="遗漏值-Missing-Values"><a href="#遗漏值-Missing-Values" class="headerlink" title="遗漏值 Missing Values"></a>遗漏值 Missing Values</h4><p>条件选择型填空常见<br>处理方法：</p><ul><li>删除数据对象或属性</li><li>估计遗漏值 平滑——连续</li><li>在分析时忽略遗漏值</li></ul><h4 id="不一致的值-Inconsistent-Values"><a href="#不一致的值-Inconsistent-Values" class="headerlink" title="不一致的值 Inconsistent Values"></a>不一致的值 Inconsistent Values</h4><p>错误的值、不在允许范围内的值</p><h4 id="重复数据-Duplicate-Data"><a href="#重复数据-Duplicate-Data" class="headerlink" title="重复数据 Duplicate Data"></a>重复数据 Duplicate Data</h4><h3 id="关于应用的问题"><a href="#关于应用的问题" class="headerlink" title="关于应用的问题"></a>关于应用的问题</h3><p>理想情况下,数据集附有描述数据的文档</p><h2 id="数据预处理-Data-Preprocessing"><a href="#数据预处理-Data-Preprocessing" class="headerlink" title="数据预处理 Data Preprocessing"></a>数据预处理 Data Preprocessing</h2><p>分为两大类:选择分析所需要的数据对象和属性 以及 创建/改变属性.</p><blockquote><p>根据习惯,使用特征(feature)或变量(variable)指代属性</p></blockquote><h3 id="聚集-Aggregation"><a href="#聚集-Aggregation" class="headerlink" title="聚集 Aggregation"></a>聚集 Aggregation</h3><p>将两个或多个对象合并成单个对象</p><p><u><strong>定量</strong> 属性(如价格)通常通过求和或求平均值进行聚集</u><br><u><strong>定性</strong> 属性(如商品)可以忽略或汇总成在一个商店销售的所有商品的集合</u></p><p>聚集的动机有多种:<br>首先,数据归约导致的较小数据集需要较少的内存和处理时间,因此,可以使用开销更大的数据挖掘算法.<br>其次,通过高层而不是低层数据视图,聚集起到了范围或标度转换的作用.<br>最后,对象或属性群的行为通常比单对象或属性的行为更稳定.</p><blockquote><p>聚集的缺点可能是丢失有趣的细节(如极值)</p></blockquote><h3 id="抽样-Sampling-Approaches"><a href="#抽样-Sampling-Approaches" class="headerlink" title="抽样 Sampling Approaches"></a>抽样 Sampling Approaches</h3><p>抽样是一种选择数据对象子集进行分析的常用方法.<br>在统计学中,抽样长期用于数据的事先调查和最终的数据分析.</p><p><strong>有效抽样的</strong> 原理:<br>如果样本是有代表性的,则使用样本与使用整个数据集的效果几乎一样.<br>而样本是有代表性的,前提是它近似地具有与原数据集相同的(感兴趣的)性质</p><h4 id="抽样方法-Sampling-Approaches"><a href="#抽样方法-Sampling-Approaches" class="headerlink" title="抽样方法 Sampling Approaches"></a>抽样方法 Sampling Approaches</h4><ul><li><p>简单随机抽样</p><ul><li>无放回抽样</li><li>有放回抽样</li></ul></li><li><p>分层抽样</p></li></ul><h4 id="渐进抽样"><a href="#渐进抽样" class="headerlink" title="渐进抽样"></a>渐进抽样</h4><p>从一个小样本开始,增加样本容量直到得到足够容量的样本.</p><h3 id="维归约-Dimensionality-Reduction"><a href="#维归约-Dimensionality-Reduction" class="headerlink" title="维归约 Dimensionality Reduction"></a>维归约 Dimensionality Reduction</h3><p>好处:</p><ul><li>如果维度较低,许多数据挖掘的算法的效果会更好.</li><li>可以使模型更容易理解</li><li>可以更容易让数据可视化</li><li>降低了数据挖掘算法的时间和内存需求</li></ul><blockquote><p>通过选择旧属性的子集得到新属性,这种归约称为 <strong>特征子集选择</strong> 或 <strong>特征选择</strong></p></blockquote><h4 id="维灾难-The-Curse-of-Dimensionality"><a href="#维灾难-The-Curse-of-Dimensionality" class="headerlink" title="维灾难 The Curse of Dimensionality"></a>维灾难 The Curse of Dimensionality</h4><p>维灾难:随着数据维度的增加,许多数据分析变得非常困难.特别是随着维度增加,数据在它所占据的空间中越来越稀疏.</p><p>结果是对于高维数据,许多分类和聚类算法(以及其他数据分析算法)都麻烦缠身–分类准确率低,聚类质量下降</p><h4 id="维归约的线性代数技术-Linear-Algebra-Techniques-for-Dimensionality-Reduction"><a href="#维归约的线性代数技术-Linear-Algebra-Techniques-for-Dimensionality-Reduction" class="headerlink" title="维归约的线性代数技术 Linear Algebra Techniques for Dimensionality Reduction"></a>维归约的线性代数技术 Linear Algebra Techniques for Dimensionality Reduction</h4><p>将数据从高维 <strong>投影</strong> 到低维空间,特别是对于 <strong>连续</strong> 数据</p><ul><li>主成分分析 Principal Components Analysis (PCA)<br>用于连续属性的线性代数技术,她找出新的属性(主成分),这些属性数原属性的线性组合,是相互 <strong>正交的 (orthogonal)</strong>,并且捕获了数据的最大变差.</li><li>奇异值分解 Singular Value Decomposition (SVD)</li></ul><h3 id="特征子集选择-Feature-Subset-Selection"><a href="#特征子集选择-Feature-Subset-Selection" class="headerlink" title="特征子集选择 Feature Subset Selection"></a>特征子集选择 Feature Subset Selection</h3><p>降低维度的另一种方法是只是用特征的一个子集</p><p><strong>Redundant features</strong> (冗余特征) duplicate much or all of the information contained in one or more other attributes.</p><p><strong>Irrelevant features</strong> (不相干特征) Oontain almost no useful information for the data mining task at hand.</p><p>冗余和不相干特征可能降低分类的准确率,影响所发现的聚类的质量.</p><p>特征选择的理想方法:将所有可能的特征子集作为感兴趣的数据挖掘算法的输入,然后选择产生最好结果的子集.</p><p>三种标准的特征选择方法:嵌入,过滤,包装</p><ul><li>嵌入方法 Embedded approaches<br>作为数据挖掘算法的一部分</li><li>过滤方法 Filter approaches<br>使用某种独立于数据挖掘的方法</li><li>包装方法 Wrapper approaches<br>这些方法将目标数据挖掘算法作为黑盒,使用类似与前面介绍的理想算法</li></ul><h4 id="特征子集选择体系结构"><a href="#特征子集选择体系结构" class="headerlink" title="特征子集选择体系结构"></a>特征子集选择体系结构</h4><p>子集评估度量,控制新的特征子集产生的搜索策略,停止搜索判断,验证过程</p><p><a target="_blank" rel="noopener" href="http://www.harumonia.top/image/NsSt"><img data-src="http://www.harumonia.top/images/2019/08/17/imagea43b43449d9493db.md.png" alt="imagea43b43449d9493db.md.png"></a></p><h4 id="特征加权-Feature-Weighting"><a href="#特征加权-Feature-Weighting" class="headerlink" title="特征加权 Feature Weighting"></a>特征加权 Feature Weighting</h4><h3 id="特征创建-Feature-Creation"><a href="#特征创建-Feature-Creation" class="headerlink" title="特征创建 Feature Creation"></a>特征创建 Feature Creation</h3><h4 id="特征提取-Feature-Extraction"><a href="#特征提取-Feature-Extraction" class="headerlink" title="特征提取 Feature Extraction"></a>特征提取 Feature Extraction</h4><p>最常使用的特征提取都是高度针对具体领域的<br>一旦数据挖掘用于一个新的领域,一个关键任务就是开发新的特征和特征提取方法</p><h4 id="映射到新的空间-Mapping-the-Data-to-a-New-Space"><a href="#映射到新的空间-Mapping-the-Data-to-a-New-Space" class="headerlink" title="映射到新的空间 Mapping the Data to a New Space"></a>映射到新的空间 Mapping the Data to a New Space</h4><p>使用一种完全不同的视角挖掘数据可能揭示出重要和有趣的特征.</p><p><strong>傅里叶变换 Fourier transform</strong></p><blockquote><p>对于时间序列和其他类型的数据,<strong>小波变换 wavelet transform</strong> 也非常有用</p></blockquote><h4 id="特征构造-Feature-Construction"><a href="#特征构造-Feature-Construction" class="headerlink" title="特征构造 Feature Construction"></a>特征构造 Feature Construction</h4><p>一个或多个由元特征构造的新特征可能比原特征更有用</p><h3 id="离散化和二元化-Discretization-and-Binarization"><a href="#离散化和二元化-Discretization-and-Binarization" class="headerlink" title="离散化和二元化 Discretization and Binarization"></a>离散化和二元化 Discretization and Binarization</h3><h4 id="二元化"><a href="#二元化" class="headerlink" title="二元化"></a>二元化</h4><p>如果有 m 个分类值,则将每个原始值唯一的赋予区间[0,m-1]中的一个整数.</p><p>这样的变换可能导致复杂化,如无意之中建立了转换后的属性之间的联系.</p><p>关联分析需要非对称的二元属性,其中只有属性的出现(值为 1)才是重要的.</p><p>对于关联问题,可能需要用两个非对称的二元属性替换单个二元属性.</p><h4 id="连续属性离散化"><a href="#连续属性离散化" class="headerlink" title="连续属性离散化"></a>连续属性离散化</h4><p>两个子任务:决定需多少个分类值,以及确定如何将连续属性值映射到这些分类值.</p><p>离散化问题就是决定选择多少个分割点和确定分割点位置的问题.</p><h5 id="非监督离散化-unSupervised-Discretization"><a href="#非监督离散化-unSupervised-Discretization" class="headerlink" title="非监督离散化 unSupervised Discretization"></a>非监督离散化 unSupervised Discretization</h5><p>聚类?</p><h5 id="监督离散化-Supervised-Discretization"><a href="#监督离散化-Supervised-Discretization" class="headerlink" title="监督离散化 Supervised Discretization"></a>监督离散化 Supervised Discretization</h5><p>基于统计学的方法用每个属性值来分区间,并通过过类似于根据统计检验得出的相邻区间来创建较大的区间.</p><p>基于熵的方法是最有前途的离散化方法之一.<br><a target="_blank" rel="noopener" href="http://www.harumonia.top/image/NHsI"><img data-src="http://www.harumonia.top/images/2019/08/17/imageb9a8cc3bce2b2a12.md.png" alt="imageb9a8cc3bce2b2a12.md.png"></a></p><p><a target="_blank" rel="noopener" href="http://www.harumonia.top/image/NfMY"><img data-src="http://www.harumonia.top/images/2019/08/17/image418cf8e9aaebd1f7.md.png" alt="image418cf8e9aaebd1f7.md.png"></a></p><h4 id="具有过多值的分类属性"><a href="#具有过多值的分类属性" class="headerlink" title="具有过多值的分类属性"></a>具有过多值的分类属性</h4><h3 id="变量变换-Variable-Tlansformation"><a href="#变量变换-Variable-Tlansformation" class="headerlink" title="变量变换 Variable Tlansformation"></a>变量变换 Variable Tlansformation</h3><p>指用于变量的所有值的变换</p><p>简单函数变换和规范化</p><h4 id="简单函数-Simple-Functions"><a href="#简单函数-Simple-Functions" class="headerlink" title="简单函数 Simple Functions"></a>简单函数 Simple Functions</h4><blockquote><p>A variable transformation refers to a transformation that is applied to all thevaluesofavariable.</p></blockquote><p>在统计学中,变量变换常用来将不具有高斯分布的数据变换成具有高斯分布的数据.<br>在数据挖掘中,可以用来压缩值域</p><h4 id="规范化和标准化-Normalization-or-Standardization"><a href="#规范化和标准化-Normalization-or-Standardization" class="headerlink" title="规范化和标准化 Normalization or Standardization"></a>规范化和标准化 Normalization or Standardization</h4><p>使整个值得集合具有特定的性质</p><h2 id="相似性和相异性的度量-Measures-of-Similarity-and-Dissimilarity"><a href="#相似性和相异性的度量-Measures-of-Similarity-and-Dissimilarity" class="headerlink" title="相似性和相异性的度量 Measures of Similarity and Dissimilarity"></a>相似性和相异性的度量 Measures of Similarity and Dissimilarity</h2><p>许多情况下,一旦计算出相似性或相异性就不再需要原始数据了.</p><p>使用属于 <strong>邻近度 proximity</strong> 来表示相似性或相异性</p><h3 id="基础-Basics"><a href="#基础-Basics" class="headerlink" title="基础 Basics"></a>基础 Basics</h3><h4 id="定义-Definitions"><a href="#定义-Definitions" class="headerlink" title="定义 Definitions"></a>定义 Definitions</h4><blockquote><p>the similarity between two objects is a numerical measure of the degreeto which the two objects are alike.</p></blockquote><blockquote><p>The dissimilarity betweentwo objects is a numerical measureof the de- gree to which the two objects are different.</p></blockquote><p>通常术语”距离”用作相异度的同义词.<br>有时相异度在[0,1]中取值,但是相异度在 0 和 ♾ 之间取值也很常见.</p><h4 id="变换"><a href="#变换" class="headerlink" title="变换"></a>变换</h4><p>通常使用变换将相似度从相异转为相反.</p><h4 id="简单属性之间的相似度和相异度"><a href="#简单属性之间的相似度和相异度" class="headerlink" title="简单属性之间的相似度和相异度"></a>简单属性之间的相似度和相异度</h4><p><a target="_blank" rel="noopener" href="http://www.harumonia.top/image/NlDw"><img data-src="http://www.harumonia.top/images/2019/08/17/imagee59e593bbcfd3120.md.png" alt="imagee59e593bbcfd3120.md.png"></a></p><h4 id="数据对象之间的相异度-Dissimilarities-between-Data-Objects"><a href="#数据对象之间的相异度-Dissimilarities-between-Data-Objects" class="headerlink" title="数据对象之间的相异度 Dissimilarities between Data Objects"></a>数据对象之间的相异度 Dissimilarities between Data Objects</h4><ul><li>距离<br><strong>欧几里得距离</strong> Euclidean distance<br><a target="_blank" rel="noopener" href="http://www.harumonia.top/image/NrLi"><img data-src="http://www.harumonia.top/images/2019/08/17/image09c78ec0185a1ef1.png" alt="image09c78ec0185a1ef1.png"></a><br>其中,n 是维数,而 <strong>x</strong> k 和 <strong>y</strong> k 分别是 x 和 y 的第 k 个属性值(分量)</li></ul><p><strong>闵可夫斯基距离</strong> Minkowski distance<br><a target="_blank" rel="noopener" href="http://www.harumonia.top/image/NzaH"><img data-src="http://www.harumonia.top/images/2019/08/17/image4bfd8ac1d8b3156f.png" alt="image4bfd8ac1d8b3156f.png"></a></p><ul><li>r=1,城市街区(也称曼哈顿,出租车,L1 范数)距离</li><li>r=2,欧几里得距离</li><li>r=♾,上确界距离</li></ul><p>度量与非度量</p><h4 id="数据对象之间的相似度-Similarities-between-Data-Objects"><a href="#数据对象之间的相似度-Similarities-between-Data-Objects" class="headerlink" title="数据对象之间的相似度 Similarities between Data Objects"></a>数据对象之间的相似度 Similarities between Data Objects</h4><h4 id="邻近性度量的例子-Examples-of-Proximity-Measures"><a href="#邻近性度量的例子-Examples-of-Proximity-Measures" class="headerlink" title="邻近性度量的例子 Examples of Proximity Measures"></a>邻近性度量的例子 Examples of Proximity Measures</h4><h5 id="二元数据的相似性度量"><a href="#二元数据的相似性度量" class="headerlink" title="二元数据的相似性度量"></a>二元数据的相似性度量</h5><ul><li>简单匹配系数 simple matching coefficient (SMC)<br><a target="_blank" rel="noopener" href="http://www.harumonia.top/image/N2RF"><img data-src="http://www.harumonia.top/images/2019/08/17/imagec78fb1c028be231c.png" alt="imagec78fb1c028be231c.png"></a></li></ul><p>SMC 可以在一个仅包含是非题的测验中来发现回答问题相似的学生</p><ul><li>Jaccard 系数 Jaccard Coefficient<br><a target="_blank" rel="noopener" href="http://www.harumonia.top/image/N4tT"><img data-src="http://www.harumonia.top/images/2019/08/17/image709b0bba0e810a01.png" alt="image709b0bba0e810a01.png"></a><br>常常用来处理非对称的二元属性对象</li></ul><h5 id="余弦相似度-Cosine-Similarity"><a href="#余弦相似度-Cosine-Similarity" class="headerlink" title="余弦相似度 Cosine Similarity"></a>余弦相似度 Cosine Similarity</h5><p>常用来度量文档相似度<br><img data-src="http://www.harumonia.top/images/2019/08/17/imagee4c8afe3e4e60ecf.png" alt="imagee4c8afe3e4e60ecf.png"><br><a target="_blank" rel="noopener" href="http://www.harumonia.top/image/NGnU"><img data-src="http://www.harumonia.top/images/2019/08/17/image950d41dec2581905.png" alt="image950d41dec2581905.png"></a></p><h5 id="广义-Jaccard-系数-Extended-Jaccard-Coefficient-Tanimoto-Coefficient"><a href="#广义-Jaccard-系数-Extended-Jaccard-Coefficient-Tanimoto-Coefficient" class="headerlink" title="广义 Jaccard 系数 Extended Jaccard Coefficient (Tanimoto Coefficient)"></a>广义 Jaccard 系数 Extended Jaccard Coefficient (Tanimoto Coefficient)</h5><p><a target="_blank" rel="noopener" href="http://www.harumonia.top/image/Nacx"><img data-src="http://www.harumonia.top/images/2019/08/17/image8002f990119f2a64.png" alt="image8002f990119f2a64.png"></a></p><h5 id="相关性-Correlation"><a href="#相关性-Correlation" class="headerlink" title="相关性 Correlation"></a>相关性 Correlation</h5><p>皮尔森相关 Pearson’s correlation<br>判定是否存在线性关系</p><p><a target="_blank" rel="noopener" href="http://www.harumonia.top/image/NhsL"><img data-src="http://www.harumonia.top/images/2019/08/17/imagefa2b122d3c3982db.png" alt="imagefa2b122d3c3982db.png"></a></p><p>Bregman 散度<br>Bregman 散度是损失或失真函数.损失函数的目的是度量用 x 近似 y 导致的失真或损失.<br>x 和 y 越类似,失真或损失就越小,因而 Bregman 散度可以用作相异性函数</p><h4 id="邻近度计算问题"><a href="#邻近度计算问题" class="headerlink" title="邻近度计算问题"></a>邻近度计算问题</h4><ol><li>当属性具有不同的尺度(scale)或相关时如何处理</li><li>当对象包含不同类型的属性(例如,定量属性和定性属性)是如何计算对象之间的邻近度</li><li>当属性具有不同的权重(即并非所有的属性都对对象的邻近度具有相等的贡献)时,如何处理邻近度计算</li></ol><h5 id="距离度量的标准化和相关性"><a href="#距离度量的标准化和相关性" class="headerlink" title="距离度量的标准化和相关性"></a>距离度量的标准化和相关性</h5><h1 id="wander-season"><a href="#wander-season" class="headerlink" title="wander season"></a>wander season</h1><h2 id="kaggle"><a href="#kaggle" class="headerlink" title="kaggle"></a>kaggle</h2><ul><li>The number of columns in the DataFrame is not equal to the number of features. One of the columns - ‘party’ is the target variable.</li></ul></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>harumonia</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://blog.harumonia.moe/2019-08-17-Introduction_to_data_mining-1.html" title="Introduction to data mining(1)">https://blog.harumonia.moe/2019-08-17-Introduction_to_data_mining-1.html</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/CC%20BY-NC-SA%204.0/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/2019-08-14-Be-a-better-pythonista-2.html" rel="prev" title="Be a better pythonista(2)"><i class="fa fa-chevron-left"></i> Be a better pythonista(2)</a></div><div class="post-nav-item"><a href="/2019-08-23-DA-intro-1.html" rel="next" title="数据分析拾遗(不定期补充)">数据分析拾遗(不定期补充) <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%AA%E8%AE%BA"><span class="nav-number">1.</span> <span class="nav-text">绪论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98"><span class="nav-number">1.1.</span> <span class="nav-text">什么是数据挖掘</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E7%9F%A5%E8%AF%86%E5%8F%91%E7%8E%B0"><span class="nav-number">1.1.1.</span> <span class="nav-text">数据挖掘与知识发现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E8%A6%81%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">1.2.</span> <span class="nav-text">数据挖掘要解决的问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E7%9A%84%E4%BB%BB%E5%8A%A1"><span class="nav-number">1.3.</span> <span class="nav-text">数据挖掘的任务</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9B%E5%A4%A7%E4%BB%BB%E5%8A%A1"><span class="nav-number">1.3.1.</span> <span class="nav-text">四大任务</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B%E5%BB%BA%E6%A8%A1"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">预测建模</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B3%E7%B3%BB%E5%88%86%E6%9E%90"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">关系分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90"><span class="nav-number">1.3.1.3.</span> <span class="nav-text">聚类分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B"><span class="nav-number">1.3.1.4.</span> <span class="nav-text">异常检测</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE"><span class="nav-number">2.</span> <span class="nav-text">数据</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="nav-number">2.1.</span> <span class="nav-text">数据类型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B1%9E%E6%80%A7"><span class="nav-number">2.1.0.1.</span> <span class="nav-text">属性</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E7%B1%BB%E5%9E%8B"><span class="nav-number">2.1.1.</span> <span class="nav-text">数据集的类型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E4%B8%80%E8%88%AC%E7%89%B9%E6%80%A7"><span class="nav-number">2.1.1.1.</span> <span class="nav-text">数据集的一般特性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%B0%E5%BD%95%E6%95%B0%E6%8D%AE"><span class="nav-number">2.1.1.2.</span> <span class="nav-text">记录数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%9B%BE%E5%BD%A2%E7%9A%84%E6%95%B0%E6%8D%AE"><span class="nav-number">2.1.1.3.</span> <span class="nav-text">基于图形的数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%89%E5%BA%8F%E6%95%B0%E6%8D%AE"><span class="nav-number">2.1.1.4.</span> <span class="nav-text">有序数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%84%E7%90%86%E9%9D%9E%E8%AE%B0%E5%BD%95%E6%95%B0%E6%8D%AE"><span class="nav-number">2.1.1.5.</span> <span class="nav-text">处理非记录数据</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F"><span class="nav-number">2.2.</span> <span class="nav-text">数据质量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%8B%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E9%97%AE%E9%A2%98"><span class="nav-number">2.2.1.</span> <span class="nav-text">测量和数据收集问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B5%8B%E9%87%8F%E8%AF%AF%E5%B7%AE%E5%92%8C%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E9%94%99%E8%AF%AF-Measurement-and-Data-Collection-Errors"><span class="nav-number">2.2.1.1.</span> <span class="nav-text">测量误差和数据收集错误 Measurement and Data Collection Errors</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%99%AA%E5%A3%B0%E5%92%8C%E4%BC%AA%E8%B1%A1-Noise-and-Artifacts"><span class="nav-number">2.2.1.2.</span> <span class="nav-text">噪声和伪象 Noise and Artifacts</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%B2%BE%E5%BA%A6-%E5%81%8F%E5%80%9A%E5%92%8C%E5%87%86%E7%A1%AE%E7%8E%87-Precision-Bias-and-Accuracy"><span class="nav-number">2.2.1.3.</span> <span class="nav-text">精度\偏倚和准确率 Precision, Bias, and Accuracy</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A6%BB%E7%BE%A4%E7%82%B9-Outliers"><span class="nav-number">2.2.1.4.</span> <span class="nav-text">离群点 Outliers</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%81%97%E6%BC%8F%E5%80%BC-Missing-Values"><span class="nav-number">2.2.1.5.</span> <span class="nav-text">遗漏值 Missing Values</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8D%E4%B8%80%E8%87%B4%E7%9A%84%E5%80%BC-Inconsistent-Values"><span class="nav-number">2.2.1.6.</span> <span class="nav-text">不一致的值 Inconsistent Values</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%87%8D%E5%A4%8D%E6%95%B0%E6%8D%AE-Duplicate-Data"><span class="nav-number">2.2.1.7.</span> <span class="nav-text">重复数据 Duplicate Data</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E4%BA%8E%E5%BA%94%E7%94%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">2.2.2.</span> <span class="nav-text">关于应用的问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-Data-Preprocessing"><span class="nav-number">2.3.</span> <span class="nav-text">数据预处理 Data Preprocessing</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%81%9A%E9%9B%86-Aggregation"><span class="nav-number">2.3.1.</span> <span class="nav-text">聚集 Aggregation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8A%BD%E6%A0%B7-Sampling-Approaches"><span class="nav-number">2.3.2.</span> <span class="nav-text">抽样 Sampling Approaches</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8A%BD%E6%A0%B7%E6%96%B9%E6%B3%95-Sampling-Approaches"><span class="nav-number">2.3.2.1.</span> <span class="nav-text">抽样方法 Sampling Approaches</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B8%90%E8%BF%9B%E6%8A%BD%E6%A0%B7"><span class="nav-number">2.3.2.2.</span> <span class="nav-text">渐进抽样</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%B4%E5%BD%92%E7%BA%A6-Dimensionality-Reduction"><span class="nav-number">2.3.3.</span> <span class="nav-text">维归约 Dimensionality Reduction</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%B4%E7%81%BE%E9%9A%BE-The-Curse-of-Dimensionality"><span class="nav-number">2.3.3.1.</span> <span class="nav-text">维灾难 The Curse of Dimensionality</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%B4%E5%BD%92%E7%BA%A6%E7%9A%84%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E6%8A%80%E6%9C%AF-Linear-Algebra-Techniques-for-Dimensionality-Reduction"><span class="nav-number">2.3.3.2.</span> <span class="nav-text">维归约的线性代数技术 Linear Algebra Techniques for Dimensionality Reduction</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%AD%90%E9%9B%86%E9%80%89%E6%8B%A9-Feature-Subset-Selection"><span class="nav-number">2.3.4.</span> <span class="nav-text">特征子集选择 Feature Subset Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%AD%90%E9%9B%86%E9%80%89%E6%8B%A9%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84"><span class="nav-number">2.3.4.1.</span> <span class="nav-text">特征子集选择体系结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%8A%A0%E6%9D%83-Feature-Weighting"><span class="nav-number">2.3.4.2.</span> <span class="nav-text">特征加权 Feature Weighting</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%88%9B%E5%BB%BA-Feature-Creation"><span class="nav-number">2.3.5.</span> <span class="nav-text">特征创建 Feature Creation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-Feature-Extraction"><span class="nav-number">2.3.5.1.</span> <span class="nav-text">特征提取 Feature Extraction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%98%A0%E5%B0%84%E5%88%B0%E6%96%B0%E7%9A%84%E7%A9%BA%E9%97%B4-Mapping-the-Data-to-a-New-Space"><span class="nav-number">2.3.5.2.</span> <span class="nav-text">映射到新的空间 Mapping the Data to a New Space</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E6%9E%84%E9%80%A0-Feature-Construction"><span class="nav-number">2.3.5.3.</span> <span class="nav-text">特征构造 Feature Construction</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A6%BB%E6%95%A3%E5%8C%96%E5%92%8C%E4%BA%8C%E5%85%83%E5%8C%96-Discretization-and-Binarization"><span class="nav-number">2.3.6.</span> <span class="nav-text">离散化和二元化 Discretization and Binarization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%8C%E5%85%83%E5%8C%96"><span class="nav-number">2.3.6.1.</span> <span class="nav-text">二元化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%9E%E7%BB%AD%E5%B1%9E%E6%80%A7%E7%A6%BB%E6%95%A3%E5%8C%96"><span class="nav-number">2.3.6.2.</span> <span class="nav-text">连续属性离散化</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9D%9E%E7%9B%91%E7%9D%A3%E7%A6%BB%E6%95%A3%E5%8C%96-unSupervised-Discretization"><span class="nav-number">2.3.6.2.1.</span> <span class="nav-text">非监督离散化 unSupervised Discretization</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%9B%91%E7%9D%A3%E7%A6%BB%E6%95%A3%E5%8C%96-Supervised-Discretization"><span class="nav-number">2.3.6.2.2.</span> <span class="nav-text">监督离散化 Supervised Discretization</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B7%E6%9C%89%E8%BF%87%E5%A4%9A%E5%80%BC%E7%9A%84%E5%88%86%E7%B1%BB%E5%B1%9E%E6%80%A7"><span class="nav-number">2.3.6.3.</span> <span class="nav-text">具有过多值的分类属性</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%98%E9%87%8F%E5%8F%98%E6%8D%A2-Variable-Tlansformation"><span class="nav-number">2.3.7.</span> <span class="nav-text">变量变换 Variable Tlansformation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E5%87%BD%E6%95%B0-Simple-Functions"><span class="nav-number">2.3.7.1.</span> <span class="nav-text">简单函数 Simple Functions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%84%E8%8C%83%E5%8C%96%E5%92%8C%E6%A0%87%E5%87%86%E5%8C%96-Normalization-or-Standardization"><span class="nav-number">2.3.7.2.</span> <span class="nav-text">规范化和标准化 Normalization or Standardization</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%92%8C%E7%9B%B8%E5%BC%82%E6%80%A7%E7%9A%84%E5%BA%A6%E9%87%8F-Measures-of-Similarity-and-Dissimilarity"><span class="nav-number">2.4.</span> <span class="nav-text">相似性和相异性的度量 Measures of Similarity and Dissimilarity</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80-Basics"><span class="nav-number">2.4.1.</span> <span class="nav-text">基础 Basics</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89-Definitions"><span class="nav-number">2.4.1.1.</span> <span class="nav-text">定义 Definitions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%98%E6%8D%A2"><span class="nav-number">2.4.1.2.</span> <span class="nav-text">变换</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E5%B1%9E%E6%80%A7%E4%B9%8B%E9%97%B4%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E5%92%8C%E7%9B%B8%E5%BC%82%E5%BA%A6"><span class="nav-number">2.4.1.3.</span> <span class="nav-text">简单属性之间的相似度和相异度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%AF%B9%E8%B1%A1%E4%B9%8B%E9%97%B4%E7%9A%84%E7%9B%B8%E5%BC%82%E5%BA%A6-Dissimilarities-between-Data-Objects"><span class="nav-number">2.4.1.4.</span> <span class="nav-text">数据对象之间的相异度 Dissimilarities between Data Objects</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%AF%B9%E8%B1%A1%E4%B9%8B%E9%97%B4%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6-Similarities-between-Data-Objects"><span class="nav-number">2.4.1.5.</span> <span class="nav-text">数据对象之间的相似度 Similarities between Data Objects</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%82%BB%E8%BF%91%E6%80%A7%E5%BA%A6%E9%87%8F%E7%9A%84%E4%BE%8B%E5%AD%90-Examples-of-Proximity-Measures"><span class="nav-number">2.4.1.6.</span> <span class="nav-text">邻近性度量的例子 Examples of Proximity Measures</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BA%8C%E5%85%83%E6%95%B0%E6%8D%AE%E7%9A%84%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F"><span class="nav-number">2.4.1.6.1.</span> <span class="nav-text">二元数据的相似性度量</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6-Cosine-Similarity"><span class="nav-number">2.4.1.6.2.</span> <span class="nav-text">余弦相似度 Cosine Similarity</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B9%BF%E4%B9%89-Jaccard-%E7%B3%BB%E6%95%B0-Extended-Jaccard-Coefficient-Tanimoto-Coefficient"><span class="nav-number">2.4.1.6.3.</span> <span class="nav-text">广义 Jaccard 系数 Extended Jaccard Coefficient (Tanimoto Coefficient)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E6%80%A7-Correlation"><span class="nav-number">2.4.1.6.4.</span> <span class="nav-text">相关性 Correlation</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%82%BB%E8%BF%91%E5%BA%A6%E8%AE%A1%E7%AE%97%E9%97%AE%E9%A2%98"><span class="nav-number">2.4.1.7.</span> <span class="nav-text">邻近度计算问题</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E7%9A%84%E6%A0%87%E5%87%86%E5%8C%96%E5%92%8C%E7%9B%B8%E5%85%B3%E6%80%A7"><span class="nav-number">2.4.1.7.1.</span> <span class="nav-text">距离度量的标准化和相关性</span></a></li></ol></li></ol></li></ol></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#wander-season"><span class="nav-number">3.</span> <span class="nav-text">wander season</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#kaggle"><span class="nav-number">3.1.</span> <span class="nav-text">kaggle</span></a></li></ol></li></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="harumonia" src="https://harumona-blog.oss-cn-beijing.aliyuncs.com/blog/Ocabe.webp"><p class="site-author-name" itemprop="name">harumonia</p><div class="site-description" itemprop="description">find the key of soul</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">101</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">16</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">53</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zxjlm" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zxjlm" rel="noopener" target="_blank"><i class="fa fa-fw fa-fab fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:zxjlm233@gmail.com" title="E-Mail → mailto:zxjlm233@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-fab fa-envelope"></i>E-Mail</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2017 – <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-fab fa-asterisk"></i> </span><span class="author" itemprop="copyrightHolder">harumonia</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">337k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">17:34</span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script><script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdnjs.cloudflare.com/ajax/libs/valine/1.3.10/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : true,
      appId      : 'MRbMQ94D5q51YRYIbsRF0kcH-gzGzoHsz',
      appKey     : 'A9JJBbjhfc9liCzCJgWIxpDa',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});</script><script async src="/js/cursor/fireworks.js"></script></body></html>